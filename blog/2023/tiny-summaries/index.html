<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta name="google-site-verification" content=""> <meta name="msvalidate.01" content=""> <meta http-equiv="Permissions-Policy" content="interest-cohort=()"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Tiny Paper Summaries | John Heyer</title> <meta name="author" content="John Heyer"> <meta name="description" content="tiny summaries/notes about things I'm reading (to help me remember them)"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link href="/assets/css/bootstrap-toc.min.css?6f5af0bb9aab25d79b2448143cbeaa88" rel="stylesheet"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%A6%86&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://john-heyer.github.io/blog/2023/tiny-summaries/"> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/">John Heyer</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">tl;dr</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/long-bio/">longish bio</a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">teaching</a> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="row"> <div class="col-sm-3"> <nav id="toc-sidebar" class="sticky-top"></nav> </div> <div class="col-sm-9"> <div class="post"> <header class="post-header"> <h1 class="post-title">Tiny Paper Summaries</h1> <p class="post-meta">December 15, 2023</p> <p class="post-tags"> <a href="/blog/2023"> <i class="fa-solid fa-calendar fa-sm"></i> 2023 </a> Â  Â· Â  <a href="/blog/tag/ml-reading"> <i class="fa-solid fa-hashtag fa-sm"></i> ml-reading</a> Â  </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Forcing myself to write (usually tiny) summaries of things Iâ€™m reading to improve retention. Not intended to teach â€“ will be simultaneously terse and verbose and probably imprecise in places!</p> <h2 id="instruction-following">Instruction following</h2> <p>Training models to follow instructions. Sometimes referred to as alignment, but thatâ€™s an overloaded term imo, and I mean something more specific here.</p> <h4 id="lima"><a href="https://arxiv.org/abs/2305.11206" rel="external nofollow noopener" target="_blank">LIMA</a></h4> <p>The authors fine-tune an LLM (Llama) on a set of just 1K curated instructions from the open assistant dataset. Their model performs comparably to GPT3.5, which was fine-tuned with many more demonstrations + RLHF, and does not trail proprietary models like GPT-4/Claude by much, despite the decrease in size and training data. They coin this phenomenon the â€œSuperficial Alignment Hypothesisâ€, as their work suggests that the vast majority of LLM capabilities are learned during pretraining rather than during alignment to human preferences via SFT/RLHF/etc. Some interesting findings include: (i) a <strong>negative</strong> correlation between validation perplexity and human-annotated response quality, and (ii) a strong ability to generalize to multi-turn without examples in training or an even stronger performance bump by including as few as 30 multi-turn conversation examples.</p> <h4 id="humpback"><a href="https://arxiv.org/abs/2308.06259" rel="external nofollow noopener" target="_blank">Humpback</a></h4> <p>The authors present a method for self-aligning language models using entirely open-source data, <strong>without</strong> distilling from data produced by stronger models (i.e., GPT4). They do so with an iterative algorithm that first (i) <strong>augments</strong> the instruction dataset by predicting instructions (\(\hat{x}\) ) from a assumed fulfillment of said instruction (response \(y\)), which is a span of text from the web. They then (ii) <strong>curate</strong> the set of augmented instruction/response pairs by prompting the model to assign scores to the quality of the instructions, and selecting only those above some score \(k\). They perform several (2) iterations of augment/curate, and then fine-tune on the augmented, higher quality subset of instruction data, \(A_{k}^{2}\). The perform exspansive analysis comparing to other instruction-following models, and demonstrate improved performance compared to any model tuned on or distilled from proprietary data.</p> <h4 id="unnatural-instructions"><a href="https://arxiv.org/abs/2212.09689" rel="external nofollow noopener" target="_blank">Unnatural Instructions</a></h4> <p>The authors create a set of instruction/input/output data by (i) prompting LLMs to generate instruction/input examples in a few-shot setting with nucleus sampling and (ii) decoding the output greedily with an LLM. Using only LLM generated instruction data, their model performs comparably to an equivalent LLM fine-tuned on â€œSupernatural Instructionsâ€, which was created using expensive human annotations. Additionally, they provide evidence that suggests that their instruction set is more diverse than those generated by humans (via BERT score), and hypothesize that humans fall back on heuristics or templates learned via annotation guidelines, which results in said lack of instruction diversity.</p> <h4 id="vicuna"><a href="https://lmsys.org/blog/2023-03-30-vicuna/" rel="external nofollow noopener" target="_blank">Vicuna</a></h4> <p>LLaMA fine-tuned on conversations from Share-GPT</p> <h4 id="wizard"><a href="https://arxiv.org/abs/2304.12244" rel="external nofollow noopener" target="_blank">Wizard</a></h4> <p>LLaMA fine-tuned using instructions of varying complexity generated by an â€œinstruction evolutionâ€ (Evol-Instruct) algorithm which prompts a strong LLM (ChatGPT) to make instructions more complex in diverse ways, then filters the pooled instruct/response examples.</p> <h4 id="alpaca"><a href="https://crfm.stanford.edu/2023/03/13/alpaca.html" rel="external nofollow noopener" target="_blank">Alpaca</a></h4> <p>Fine-tuned LLaMA using â€œ<a href="https://arxiv.org/pdf/2212.10560.pdf" rel="external nofollow noopener" target="_blank">self-instruct</a>â€ generations produced by ChatGPT given a seed set of instructions.</p> <h2 id="rag--agents">RAG &amp; Agents</h2> <p>Getting models to respond using tools + data (potentially) not seen during training (i.e., non-parametrically).</p> <h4 id="toolformer"><a href="https://arxiv.org/abs/2302.04761" rel="external nofollow noopener" target="_blank">Toolformer</a></h4> <p>The authors detail a fully self-supervised method for teaching an LLM to (selectively) use tools, including calculators, search engines and calendars. They do so by fine-tuning a model on a modified pretraining corpus that includes API input/output results in plain text when relevant. They insert such API calls by (i) few-shot prompting an LLM with examples that demonstrate when an API call is useful(e.g., â€œOne US dollar is approximately equal to 23 Czech crown, or <code class="language-plaintext highlighter-rouge">[API(1/23)â†’.043]</code> 4.3%.â€), and (ii) filtering to only cases when the API input/output reduces the complexity of the subsequent tokens by at least some fixed amount (hyperparameter). Their model, Toolformer, is better than models which cannot utilize APIs on several tasks including math, question answering, and time-sensitive QA, and even exceeds performance of the much larger GPT3 on several tasks.</p> <h4 id="self-rag"><a href="https://arxiv.org/pdf/2310.11511.pdf" rel="external nofollow noopener" target="_blank">Self-RAG</a></h4> <p>Learning <em>when</em> to retrieve + critiquing retrieved results generatively.</p> <h4 id="react"><a href="https://arxiv.org/abs/2210.03629" rel="external nofollow noopener" target="_blank">ReAct</a></h4> <p>Prior work has explored:</p> <ol> <li>using language models to generate actions plans (e.g., <a href="https://arxiv.org/abs/2112.09332" rel="external nofollow noopener" target="_blank">WebGPT</a>)</li> <li>improving â€œreasoningâ€ capabilities of language models via prompting strategies such as <a href="https://arxiv.org/abs/2201.11903" rel="external nofollow noopener" target="_blank">CoT</a>. ReAct combines these 2 directions with a prompting â€œframeworkâ€ that uses in context learning (ICL) to describe the action space (descriptions of actions/tools/apis) and encourage the model to generate â€œthoughtsâ€ prior to generating action plans. Their hypothesis is that using thoughts will improve reasoning, i.e., action selection in this setting. Their experiments on multi-hop QA, fact-checking, and other interactive decision-making tasks support this.</li> </ol> <p>The authors also explore fine-tuning LLMs on react trajectories (sequences of (thought, action, observation) pairs). They automatically create SFT data via a bootstrapping method in which they use ReAct via few-shot prompting an LLM (not fine-tuned) and using <em>only</em> the trajectories that result in correct final responses (using heuristics like exact match from NQ, for example) for fine-tuning (how biased is this sample?). This also results in improved performance, but at what cost? To what extent are we compromising general language modeling capabilities when we fine-tune on data, most of which essentially only requires solving a classification task (thought/action selection) to model well?</p> <h4 id="rewoo"><a href="https://arxiv.org/abs/2305.18323" rel="external nofollow noopener" target="_blank">ReWOO</a></h4> <p>Removes the sequential (or â€œreactiveâ€ <img class="emoji" title=":wink:" alt=":wink:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f609.png" height="20" width="20">) planning of ReAct by decoupling the planning from the execution of actions. They do so by generating plans with intermediate results referenced via variables that can be used in subsequent actions (see example in fig below). This is more efficient (time and $), because it requires fewer invocations of the LLM to generate action plans.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/sci-figs/rewoo-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/sci-figs/rewoo-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/sci-figs/rewoo-1400.webp"></source> <img src="/assets/img/sci-figs/rewoo.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Overview of ReWOO from (<a href="https://arxiv.org/pdf/2305.18323.pdf" rel="external nofollow noopener" target="_blank">Xu et al.</a>). </div> <h4 id="llm-compiler"><a href="https://arxiv.org/abs/2312.04511" rel="external nofollow noopener" target="_blank">LLM Compiler</a></h4> <p>Nice work, but glorified ReWOO that fails to give it enough credit. They extend ReWOO by generating a directed acyclic graph (DAG) with their action planner. This allows for parallel execution of independent tasks. This is not really a limitation of ReWOO, just something that the authors did not experiment with. They even mentioned concurrent execution of DAGs in their future work!</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/sci-figs/llm-compiler-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/sci-figs/llm-compiler-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/sci-figs/llm-compiler-1400.webp"></source> <img src="/assets/img/sci-figs/llm-compiler.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Overview of LLM Compiler from (<a href="https://arxiv.org/pdf/2312.04511.pdf" rel="external nofollow noopener" target="_blank">Kim et al.</a>). </div> <h2 id="parameter-efficient-fine-tuning-peft">Parameter-efficient fine-tuning (PEFT)</h2> <p>Fine-tuning models with fewer learned parameters!</p> <h4 id="lora"><a href="https://arxiv.org/abs/2106.09685" rel="external nofollow noopener" target="_blank">LoRA</a></h4> <p>The authors fine-tune large models by freezing them and only training small adapters. They do so by decomposing a large matrix \(W'\) into a sum of the (frozen) pretrained matrix \(W_0\) and a low-rank perturbation \(\Delta W\). Thus the forward pass to compute hidden state \(h\) from input \(x\), \(h = W_0x\), is replaced with: \begin{equation} h = W_0x + \Delta W_x = W_0x + BAx. \label{lora-eq1} \end{equation} Importantly, \(\Delta W\) is low rank because it is factorized into matrices \(A\) and \(B\) which are dimension \(d_{model} \times r\), where r is something small like 8. Further, after training, these matrices can be â€œmergedâ€ into a matrix \(W'\) thatâ€™s the same size as the original matrix \(W_0\), so thereâ€™s no increase in computation/latency at inference time!</p> <p>During training, they update as few as &lt; 1% of the total parameters, which saves a ton on memory, because optimizers like <a href="https://arxiv.org/abs/1412.6980" rel="external nofollow noopener" target="_blank">Adam</a> require storing optimizer states proportional in memory to the number of tuned parameters, in order to compute adaptive gradient updates from biased estimates of gradient mean/variance through time.</p> <div class="row mt-3 justify-content-center"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/sci-figs/lora-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/sci-figs/lora-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/sci-figs/lora-1400.webp"></source> <img src="/assets/img/sci-figs/lora.png" class="img-fluid rounded z-depth-1" width="50%" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Overview of LoRA from (<a href="https://arxiv.org/pdf/2106.09685.pdf" rel="external nofollow noopener" target="_blank">Hu et al.</a>). Only A and B matrices are trained. </div> <h4 id="qlora"><a href="https://arxiv.org/pdf/2305.14314.pdf" rel="external nofollow noopener" target="_blank">QLoRA</a></h4> <p>The authors fine-tune Llama models using LoRA adapters atop 4 and 8 bit quantized base LLMs without sacrificing performance. Additionally, they fine-tune their model using instruction/chat datasets like open assistant/flanv2 and demonstrate performance comparable to ChatGPT as judged by GPT-4, highlighting the importance of data quality over quantity. Impressively, this can be done using only a single A100 GPU.</p> <h2 id="understanding-transformer-mechanisms">Understanding Transformer Mechanisms</h2> <p>Understanding <em>how</em> transformers implement things (mechanistic interpretability) and other transformer-specific stuff.</p> <h4 id="a-mathematical-framework-for-transformer-circuits"><a href="https://transformer-circuits.pub/2021/framework/index.html" rel="external nofollow noopener" target="_blank">A Mathematical Framework for Transformer Circuits</a></h4> <p>Inspired by the <a href="https://distill.pub/2020/circuits/" rel="external nofollow noopener" target="_blank">Distill ciruits thread</a>, which defined &amp; investigated â€œcircuitsâ€, or weights of a neural network that connect such that they implement some interpretable â€œfunctionâ€ like detecting curves in images, the authors set out to study circuits in transformer models. They do so by first conceptualizing transformers in a new â€“ but mathematically equivalentâ€“ way, then finding circuits in â€œtoyâ€ models (0 to 2-layer transformers).</p> <p><u><b>Conceptual Framework</b></u></p> <p><strong>Residual Stream</strong>: This work uses the â€œresidual streamâ€ to refer to the contextualized embeddings (or hidden states) produced by the transformer. They use this term for a single-token embedding and sequences of tokens alike, and throughout all layers of the model, akin to a â€œstateâ€ vector that changes at each layer. They prefer this term because it emphasizes the residual nature, i.e., that each layerâ€™s output is the sum of its input and some transformation of that input (e.g., \(t_{i+1} = t_{i} + f(t_i)\)). They like to think of attention heads functionally â€œreadingâ€ from and â€œwritingâ€ information to the residual stream.</p> <p><strong>Independent + Additive Attention Heads</strong>: The original Transformers paper (<a href="https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf" rel="external nofollow noopener" target="_blank">Vaswani et al.</a>) parameterizes an attention â€œheadâ€ by three matrices: \(W_Q\), \(W_K\), \(W_O\), which represent the â€œqueryâ€, â€œkeyâ€, and â€œvalueâ€ weights, respectively. The query and key matrices are used to compute the attention pattern, and the â€œresultâ€, \(r^{h_1}\) for head 1, is the attention-weighted sum of the value vectors. The \(n\) result vectors (1 per head) are <em>concatenated</em> (\([r^{h_1}, ..., r^{h_n}]\)), before being multiplied by a final â€œoutputâ€ matrix \(W_O\). Instead, we can split the output matrix into \(n\) â€œblocksâ€, each of size \(d_{\text{model}} \times d_{\text{model}} / n\), and express this as a sum of products with each block:</p> <p>\begin{align} \label{framework} \begin{bmatrix} r^{h_1} \cr \vdots \cr r^{h_n} \end{bmatrix} = [W_O^{h_1}, â€¦, W_O^{h_n}] \cdot \begin{bmatrix} r^{h_1} \cr \vdots \cr r^{h_n} \end{bmatrix} = \sum_i^n{W_O^{h_i} r^{h_i}}, \end{align}</p> <p>which makes it clear that each attention head <em>independently</em> contributes to the information added to (or removed from) the residual stream. Additionally, we can (conceptually) parameterize each attention â€œheadâ€ with its own â€œoutputâ€ weights \(W_O^{h_n}\).</p> <p><strong>\(W_{QK}\) &amp; \(W_{OV}\) Matrices</strong>: The authors find it useful to decompose attention heads into two independent operations: i) producing attention patterns, and ii) determining what information to read from source tokens and how to write it to destination tokens. Traditionally, we compute attention patterns by computing â€œqueryâ€ and â€œkeyâ€ values separately, then computing their inner-products, but this is equivalent to multiplying by the low-rank \(W_{QK}\) matrix. Similarly, the â€œoutputâ€ and â€œvalueâ€ matrices always operate together, and given an attention pattern, we can compute the headâ€™s output by multiplying with the low-rank \(W_{OV}\) matrix.</p> <p><strong>QK &amp; OV Circuits</strong>: When studying 1-layer transformer models, we can compute what theyâ€™ve called the â€œQKâ€ or â€œOVâ€ <em>circuits</em> (or matrices). This is best explained via image :):</p> <div class="row mt-3 justify-content-center"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/sci-figs/mech-qk-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/sci-figs/mech-qk-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/sci-figs/mech-qk-1400.webp"></source> <img src="/assets/img/sci-figs/mech-qk.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> QK and OV circuits from (<a href="https://transformer-circuits.pub/2021/framework/index.html#summary-of-results" rel="external nofollow noopener" target="_blank">Elhage et al.</a>). </div> <p>These 4-matrix products (circuits) each produce \(d_{vocab} \times d_{vocab}\) dimensional matrices, which can be interpreted! The â€œQKâ€ circuit describes how much a query token â€œwantsâ€ to attend to another key token, while the â€œOVâ€ circuit describes how much a given token (if attended to) will change the logits corresponding to another output token. The authors analyze these circuits by literally reading the billions of entries in these matrices and finding outstanding entries.</p> <p><u><b>Mechanisms</b></u></p> <p><strong>Zero layer transformers</strong>: â€œZero layer transformers model bigram statistics.â€</p> <p><strong>One layer transformers</strong>: â€œOne layer attention-only transformers are an ensemble of bigram and â€œskip-trigramâ€ (sequences of the form â€œAâ€¦ B Câ€) models.â€</p> <p><strong>Two layer transformers</strong>: â€œTwo layer attention-only transformers can implement much more complex algorithms using compositions of attention heads.â€</p> <p><strong>Induction Heads</strong>: Two-layer models ostensibly learn â€œinduction headsâ€ which allow us to model patterns of the form: <code class="language-plaintext highlighter-rouge">[a][b] â€¦ [a] â†’ [b]</code>. This can be thought of as the simplest form of in-context learning, where a model learns a pattern observed previously in the sequence. In fact, this generalizes even for random sequences, indicating that these patterns werenâ€™t simply memorized during training. Modeling this requires â€œcompositionâ€ of attention heads, where one head in the first layer copies information from the previous token in the sequence, and another head in the next later searches for â€œsimilarâ€ queries and finds the token which it should predict next (<code class="language-plaintext highlighter-rouge">b</code>), because the residual stream corresponding to this token contained information indicating that <code class="language-plaintext highlighter-rouge">a</code> was its previous token!</p> <p><strong>Final Note</strong></p> <p>This paper is <em>super</em> long, but thereâ€™s loads of great stuff in here that I canâ€™t do justice in a summary, so again this work warrants its own post. The work took me tons of time to read, and then some more time to read againâ€¦ but Iâ€™d really recommend it to anyone looking to understand transformers deeply, which often requires a new perspective.</p> <h4 id="what-learning-alg-is-in-context-learning"><a href="https://arxiv.org/abs/2211.15661" rel="external nofollow noopener" target="_blank">What learning alg is in-context learning?</a></h4> <p>The authors demonstrate that via in context learning, transformers are capable of implementing â€œclassicâ€ learning algorithms like using OLS or SGD to solve their prototypical linear regression problems.</p> <h2 id="safety">Safety</h2> <p>Understanding + mitigating model biases, frameworks for evaluating models on hard tasks, red-teaming, the works.</p> <h4 id="towards-understanding-sycophancy"><a href="https://arxiv.org/abs/2310.13548" rel="external nofollow noopener" target="_blank">Towards Understanding Sycophancy</a></h4> <p>Studies the prevalence of sycophancy, or flattery in order to gain the approval of a superior, in language models, particularly those trained with human preferences. They find several patterns of sycophantic behavior such as:</p> <ul> <li>admitting mistakes that donâ€™t exist when questioned by the user</li> <li>demonstrating biases that align with the user.</li> </ul> <p>Really nice figs, so Iâ€™m including several directly from the paper:</p> <div class="row mt-3 justify-content-center"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/sci-figs/syc1-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/sci-figs/syc1-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/sci-figs/syc1-1400.webp"></source> <img src="/assets/img/sci-figs/syc1.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/sci-figs/syc2-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/sci-figs/syc2-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/sci-figs/syc2-1400.webp"></source> <img src="/assets/img/sci-figs/syc2.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="row mt-3 justify-content-center"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/sci-figs/syc4-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/sci-figs/syc4-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/sci-figs/syc4-1400.webp"></source> <img src="/assets/img/sci-figs/syc4.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/sci-figs/syc3-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/sci-figs/syc3-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/sci-figs/syc3-1400.webp"></source> <img src="/assets/img/sci-figs/syc3.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> [<b>Click to zoom</b>] Demonstrations of models i) providing biased feedback, ii) swaying opinions, iii) conforming to a user's preconception, and iv) mimicking users' mistakes from (<a href="https://arxiv.org/pdf/2310.13548.pdf" rel="external nofollow noopener" target="_blank">Sharma et al.</a>) </div> <p>They then prompt a model to generate features for a given <code class="language-plaintext highlighter-rouge">(user_input, preferred_response, dispreferred_response)</code> interaction, and use these features as input to a Bayesian logistic regression model trained to predict which response was preferred (by humans) from these features alone. They find that agreeing with the user is one of the most predictive features of preference.</p> <p>The authors investigate whether preference models (PMs, trained to predict human preferences) encourage sycophancy. They use the PM used to train Claude 2 to select the best response from N sampled generations. They also do the same with a â€œNon-sycophantic PMâ€, which simply prepends additional instructions to the PM to discourage sycophancy explicitly. They find that â€œfeedback sycophancyâ€ correlates positively with PM score, but â€œanswerâ€ and â€œmimicryâ€ sycophancy correlate negatively. In all cases, the non-sycophantic PM prefers less sycophantic responses than the baseline PM. Additionally, they measure how these sycophancy metrics change during RL training, and find that â€œfeedbackâ€ and â€œmimicryâ€ sycophancy increase with training steps, but â€œanswerâ€ sycophancy remains flat.</p> <div class="row mt-3 justify-content-center"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/sci-figs/syc5-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/sci-figs/syc5-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/sci-figs/syc5-1400.webp"></source> <img src="/assets/img/sci-figs/syc5.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Preference Models might encourage sycophancy! From (<a href="https://arxiv.org/pdf/2310.13548.pdf" rel="external nofollow noopener" target="_blank">Sharma et al.</a>) </div> <p>They conclude with a study abot how often humans/PMs prefer sycophantic responses to truthful ones (that might correct a userâ€™s misconception), and find that both humans and PMs prefer sycophantic responses to truthful ones at times. They cleverly construct a dataset of questions around common misconceptions from TruthfulQA (e.g., â€œGeorgia produces the most peaches in the U.S.â€), and prompting a model to produce 1) sycophantic responses that agree with the user and 2) truthful responses that correct the user.</p> <div class="row mt-3 justify-content-center"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/sci-figs/syc6-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/sci-figs/syc6-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/sci-figs/syc6-1400.webp"></source> <img src="/assets/img/sci-figs/syc6.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Humans and PMs alike sometimes prefer sycophancy. From (<a href="https://arxiv.org/pdf/2310.13548.pdf" rel="external nofollow noopener" target="_blank">Sharma et al.</a>) </div> <p>I realize this is way more than a <em>tiny</em> summary, but because this work is really extensive and empirical, it doesnâ€™t have a <em>single</em>, easily-summarized core idea/method. Perhaps this deserves its own post :)</p> <p><strong>Why I like this work</strong></p> <p>I think this work is exciting because it provides a way of quantifying <em>systematic</em> failures of LLMs. There are many papers recently that make claims about what LLMs allegedly <em>canâ€™t</em> do (e.g., reasoning), but too often the goal often seems to be to criticize existing methods or claims without offering solutions. This work instead seeks to identify situations in which LLMs stray from the truth, but suggests that there are ways to mitigate this issue! Further, sycophancy in models is intuitive and perhaps not surprising, but measuring it is quite complicated; this paper proposes creative ways of designing experiments to analyze a specific behavior. Iâ€™m hopeful that additional work in identifying, and root causing, other systematic failures might lead to measurable improvements toward reliable and truthful assistants. My hunch is that preference models optimize for â€œstyleâ€ which is often at odds with <em>truth</em>, suggesting that we need to find ways to model for this outside of simply scaling data and compute.</p> <p><strong>Meta Sycophancy (to bit, or not to bit)</strong></p> <p>I originally found this work because Ethan Perez, one of the leads, <a href="https://x.com/EthanJPerez/status/1717288496279519273?s=20" rel="external nofollow noopener" target="_blank">tweeted about it</a>. I really like empircal studies like this one, because theyâ€™re packed with lots of experiments/findings/insights, as opposed to works with a single-but-clear contribution. Obviously, those works are just as, if not more, â€œimportantâ€, but empirical work like this feels rare to me. Perhaps this is because it requires a sort of privilege to be able to conduct such a study (â³ + ğŸ’°), but I think thereâ€™s more to it than that â€“ to endeavor to do work that doesnâ€™t ~directly push the SOTA~ takes courage.</p> <p>Anyway, I found out about and ultimately applied to Ethanâ€™s <a href="https://www.matsprogram.org/language" rel="external nofollow noopener" target="_blank">MATS Stream</a> because of this. There was a question in the application form asking you to describe a recent work you liked and why you found it exciting. Given my path to finding out about the program, a response detailing my interest in Ethanâ€™s work on sycophancy would be genuine, of course, but surely would reek of sycophancy without such context! However, I was super amused by the idea of asking Claude to describe the work and tell me why it was exciting, and couldnâ€™t help myself:</p> <div class="row mt-3 justify-content-center"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/sci-figs/meta-syc-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/sci-figs/meta-syc-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/sci-figs/meta-syc-1400.webp"></source> <img src="/assets/img/sci-figs/meta-syc.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> ~Meta Sycophancy~ </div> <p>I was not accepted into the program, so perhaps Ethan did not find it as amusing as I did, but Iâ€™d rather live life thinking otherwise and assume he got at least a chuckle out of it. The application process was a nice experience for me either way, because it forced me to engage deeply with this work.</p> <h4 id="measuring-progress-on-scalable-oversight"><a href="https://arxiv.org/abs/2211.03540" rel="external nofollow noopener" target="_blank">Measuring Progress on Scalable Oversight</a></h4> <p>I havenâ€™t delved into the scalable oversight literature at all â€“ so my takes here are very â€œrawâ€. I think in this case it was a useful thought exercise to try to explore this work without pre-exposure and try to come to my own conclusions (or really just more questions). Given that this is the first empirical study on oversight, many less-than-ideal decisions are made, allowing for substantial room for improvement, but also an opportunity to wrestle with yourself trying to understand if there are any confounding variables. Further, I found it hard to extrapolate from the findings of this work into the realm of significantly increased model capabilities that exceed expert humans. As a mere human, I would love to hear the opinions of experts.</p> <p><strong>What is scalable oversight?</strong></p> <p><em>Oversight</em> here refers to the ways in which we supervise models â€“ either through labels, feedback, rewards, etc. So what does it mean to oversee in a scalable way? Scalable along what axis? In this context, <em>scalable</em> qualifies oversight that extends to increasingly difficult <em>tasks</em>. Itâ€™s quite unclear how to supervise a model that exceeds human performance, and itâ€™s equally unclear how to research such models, as they donâ€™t yet exist. This work proposes ways to simulate this setting with existing models, and evaluate the extent to which we can oversee models that exceed our performance. If our oversight techniques succeed in a somewhat contrived setting, this increases our confidence that they might work in the super-intelligent-wild.</p> <p><strong>ğŸ¤–<img class="emoji" title=":sandwich:" alt=":sandwich:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f96a.png" height="20" width="20"></strong></p> <p>In the â€œsandwichâ€ setting, a task is chosen on which models <em>exceed</em> typical human performance, but perform worse than experts (ğŸ‘¨â€ğŸ”§ &lt; ğŸ¤– &lt; ğŸ‘©ğŸ½â€ğŸ”¬). Non-expert humans must oversee models to increase their performance on this task, but they cannot collaborate with experts; experts are only used to evaluate final model performance.</p> <p><strong>Wut da <em>hell</em> is alignment?</strong></p> <p>The authors define alignment by contrasting it with capability. They say a language-model-based system is <em>capable</em> if it can be made to perform a task with small interventions such as in-context learning or fine-tuning. The goal of alignment is to produce a model that can perform a task <em>without</em> such interventions <em>at test time</em>. When they say a model is <em>misaligned</em>, it implies that it is capable. <strong>Q</strong>: Do prompt engineering strategies like â€œsystemâ€ prompts or CoT count as interventions? If the model performs well with these <em>cleverly designed</em> but still zero-shot instructions, is it aligned?</p> <p>This is closer to the â€œinstruction followingâ€ variant of alignment, where the goal is to point a capable model in the â€œrightâ€ direction, where right means objectively correct in this case, rather than aligned with the â€œvaluesâ€ of humans (or some subset of them). Importantly, we can measure alignment in this setting using input and output only, rather than requiring some transparent model of the decision making-process. For truly super-intelligent models, weâ€™d no longer have the means to evaluate in this way. What are some useful hypothetical tasks that models could conceivably solve that humans cannot (genuinely curious; not a hypothetical)? Looking at super-intelligent systems today, like chess-bots and other systems requiring deep-seach, we have the means to evaluate a botâ€™s skill, because we can observe the result of a game against an opponent. Alternatively, if a language model were to solve one of the famous â€œmillion dollar proofsâ€ eluding mathematicians, it would have to do so via first principles, thus would be verifiable. If a model were to discover a cancer-curing drug, we could conduct tests to verify its efficacy. Presumably, solving these tasks requires clever methods of search + verify, but the problem is, we donâ€™t have ways to supervise this process because we canâ€™t solve any of these tasks ourselves despite the fact that we can evaluate them. What are examples of tasks that we might not even be able to evaluate?</p> <p><strong>Experimental Setup</strong></p> <p>The authors compare models, humans, and model-human teams on two tasks: MMLU (hard multiple choice questions from various topics) and QuALITY (timed, long passage QA). In their experiments, they remove the constraint that experts are only used to evaluate <em>at the end</em>, but instead involve experts after each <em>inner</em> iteration, or attempt at aligning a model. â€œExpertsâ€ here are not used directly, because they only study multiple choice tasks, with discrete labels (presumably produced by experts at some point, however). They also do not allow for fine-tuning models, and only explore strategies in which humans can interact with the model through chat. Thus, the oversight methods explored (and the methods cited under â€œPotential Techniquesâ€), require that the model is already â€œalignedâ€ on tasks that we <em>can</em> oversee, such as dialog. Specifically, their base model has been trained to perform â€œchatâ€, which requires following instructions and maintaining context over multiple turns. Where do we draw the line for â€œmisalignedâ€ models, and how do assumptions about already-aligned capabilities muddy conclusions? In theory, training a dialog assistant with RLHF entails aligning a model to perform a superset of the tasks studied here (though, these tasks might be in the tail in terms of difficulty).</p> <p><strong>Results</strong></p> <p>ğŸ‘¨â€ğŸ”§ &lt; ğŸ¤– &lt; (ğŸ‘¨â€ğŸ”§ + ğŸ¤–) &lt; ğŸ‘©ğŸ½â€ğŸ”¬: humans and models are complementary</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/sci-figs/oversight-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/sci-figs/oversight-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/sci-figs/oversight-1400.webp"></source> <img src="/assets/img/sci-figs/oversight.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Results from (<a href="https://arxiv.org/pdf/2211.03540.pdf" rel="external nofollow noopener" target="_blank">Measuring Scalable Oversight.</a>). </div> <p><strong>Conclusions, Questions, Implications</strong></p> <p>The authors provide <em>empirical</em> evidence through cleverly-designed experiments that model/human teams can outperform either component in isolation. They are clear in the limitations of their experiments, such as their relaxations of the sandwich setting. Additionally, they state plainly that the techniques revealed in their study are far from good enough to safely oversee extremely-capable models. They hope that their study will lay the ground work for future empirical studies of other oversight techniques.</p> <p>Iâ€™m somewhat conflicted about the experiments on QuALITY â€“ humans perform poorly due to an imposed time constraint, which obviously does not impact models. This is different from the inability to recall/compose knowledge (or lack thereof) suffered by <em>both</em> humans and models to varied extent on MMLU. Humans <em>are already experts</em> (best untimed human performance is between 86-94%, exceeding the best model + human performance here) â€“ they can effectively â€œoverseeâ€ models just by taking their time <img class="emoji" title=":smile:" alt=":smile:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f604.png" height="20" width="20">.</p> <p>Instead of including a human-in-the-loop to â€œperform task more accuratelyâ€ â€“ as in MMLU â€“ itâ€™s â€œhelp me perform this task quickly, without sacrificing too much accuracyâ€. Compared to humans, even the largest of models are already <em>good at fast</em>. This task might only be a proper sandwich under a relatively small window of time constraints. I donâ€™t mean to argue that these techniques are not useful â€“ doing tasks quickly has its merits â€“ but how might you transfer these demonstrations to produce an aligned model (i.e., one that performs better on the task <em>without</em> further intervention)? Letâ€™s assume that we have a way to effectively transfer this â€œpolicyâ€ learned in the sandwich setting to models (i.e., <em>align</em>) such that their capability will match that of the human + model team <em>without</em> the human (e.g., we can collect enough demonstrations using our efficacious human/model policy until we can induce this behaviour). Surely, youâ€™d choose the oversight strategy of taking your time reading the passage to maximize accuracy, over the best strategy learned under some time constraint (though, the latter strategy is much more â€œscalableâ€ w.r.t. time-of-manual-labor), because the time-constraint poses no threat to the aligned model at test time.</p> <p>Now, letâ€™s revisit the assumption that we can align a model from these methods of oversight â€“ how might we actually use these strategies to improve models? Letâ€™s ignore the fact that humans were presented with expert answers during the â€œinner loopâ€, and assume this was a â€œtrue sandwichâ€. Weâ€™ve discovered an effective protocol by interacting with models through chat, and have learned through experts that our protocol is sufficiently accurate (~about expert performance). We can now employ whatever supervised fine-tuning strategies we have at our disposal, because we effectively have expert-labeled data. The assumption that our <em>protocol</em> will scale to even harder tasks, is too strong. I donâ€™t think thatâ€™s implied here, but it could certainly be interpreted that way. Itâ€™s important to note that the humans â€“ who are the least capable at performing the task at hand â€“ discovered ways in which they could <em>improve upon</em> a modelâ€™s performance <em>without</em> verifying that their strategy was any good (this is not actually true, because they told participants if they were correct/incorrect, but they found that this did not result in progressively better performance, so letâ€™s ignore that). This means that the humans were able to leverage strengths of these models (e.g., factual recall) to complement their own reasoning in such a way that they were <em>convinced</em> that their protocol was good (in some sense, they became â€œconfidentâ€ in their answers). I think the implication here is still important â€“ if humans can find <em>some</em> protocol using models to improve their performance on a task, then perhaps we can partner with models on even harder tasks as they become increasingly capable. Neither us, nor the model alone can solve the math proof, but perhaps together we can push the frontier ever-closer. But maybe Iâ€™m still missing the point?</p> <h2 id="rl--human-preferences">RL + Human Preferences</h2> <p>Learning from human preference data?!</p> <h4 id="rlhf-instruct-gpt"><a href="https://arxiv.org/pdf/2203.02155.pdf" rel="external nofollow noopener" target="_blank">RLHF (Instruct GPT)</a></h4> <p>Collect preference data by asking humans which completions they prefer â€” can be pairwise, or a full ranking over k completions. Using this preference data, train a reward model to compute scores for each completion independently. This score is used in the loss function by taking k choose 2 pairs and ensuring the preferred generation A is given a higher score than less-preferred generation B as follows:</p> \[\text{Loss}(\theta) = -1/(k \text{c} 2)\mathbb{E}[\text{log}(\sigma(r_{\theta}(x, y_{win}) - r_{\theta}(x, y_{loss})))]\] <p>Then the model is trained using RL, PPO specifically, to maximize the reward for <em>new</em> completions to inputs by sampling completions and updating the policy to maximize expected reward returned by the reward model. Additionally, there is a KL-divergence component of the loss wrt the original model (â€policyâ€), to regularize the model to ensure it retains most of the pre-trained knowledge.</p> <h4 id="ppo"><a href="https://openai.com/research/openai-baselines-ppo" rel="external nofollow noopener" target="_blank">PPO</a></h4> <p>Policy-gradient RL learning alg (sample trajectories, then compute the gradient wrt the expected reward). Has some additional features to reduce the variance of the gradient vector (variance in gradient update is measured across minibatches? increasing batch-size reduces variance between mini-batches â€” rl suffers more from this problem because there isnâ€™t direct supervision, but rather random trajectories through the environment).</p> <h4 id="dpo"><a href="https://arxiv.org/abs/2305.18290" rel="external nofollow noopener" target="_blank">DPO</a></h4> <p><strong>tl;dr</strong>: Shows that you can reformulate the reward maximizing objective used in RLHF as a function of your current parameterized policy (i.e., your language model). In essence, learn to assign higher log likelihood to the preferred completion than the non-preferred one, as opposed to training a separate reward model.</p> <p><strong>From RLHF to the DPO Objective</strong></p> <ol> <li>Assume human preferences are generated by some latent reward model, \(r^*(x, y)\)</li> <li> <p>The Bradley-Terry preference model defines the preference distribution as a function of these rewards: \begin{equation} \label{dpo-eq1} p(y_1 \succ y_2 | x) = \frac{\text{exp}(r^{*}(x, y_1))}{\text{exp}(r^{*}(x, y_1)) + \text{exp}(r^{*}(x, y_2))} \end{equation}</p> </li> <li> <p>RLHF first trains a reward model to maximize the difference between the rewards assigned to the winning and losing completions: \begin{equation} \label{dpo-eq2} \text{Loss}(\phi) = -\mathbb{E}[\text{log}\sigma(r_{\phi}(x, y_{w}) - r_{\phi}(x, y_{l}))] \end{equation}</p> </li> <li> <p>Then trains a language model to maximize the rewards from this model: \begin{equation} \label{dpo-eq3} \text{max}_{\pi_{theta}} \mathbb{E}[r_{\phi}(x, y)] -\beta \text{D}_{\text{KL}} [\pi_{theta}(y | x) || \pi_{\text{ref}}(y | x)], \end{equation} where \(\text{D}_{\text{KL}}\) is the KL-divergence between 2 distributions, defined by the current policy and the â€œreferenceâ€ policy, typically defined by the model prior to RLHF training (i.e., the initialized model). This is a form of regularization that also prevents mode collapse.</p> </li> <li> <p>DPO first shows that the optimal solution to the RLHF objective has the form: \begin{equation} \label{dpo-eq4} \pi_{r}(y | x) = \frac{1}{Z(x)} \pi_{\text{ref}}(y | x) \text{exp}(\frac{1}{\beta}r(x, y), \end{equation} where \(Z(x)\) is the partition function, or normalizing constant required to convert the above into a valid probability distribution. This is intractable to compute because itâ€™d require summing over all generations \(y\).</p> </li> <li> <p>We can now re-write the reward as a function of its optimal policy: \begin{equation} \label{dpo-eq5} r(x, y) = \beta \text{log} \frac{\pi_r (y | x)}{\pi_{ref} (y | x)} + \beta \text{log} Z(x) \end{equation}</p> </li> <li> <p>There are 2 important things here: i) the above is a function of the optimal <em>policy</em>, which we parameterize directly (the LLM), rather than a separate reward model, and ii) the partition term cancels out when we take the difference of two rewards, which is all that matters for learning a reward model from preference data. Now, we can plug this reward difference into the reward modeling objective equation \eqref{dpo-eq2}, and optimize this by tuning our generative modelâ€™s weights directly: \begin{equation} \label{dpo-eq6} \mathcal{L}_{\text{DPO}} = -\mathbb{E}\Bigl[\text{log}\sigma\Bigl(\beta \text{log} \frac{\pi_{\theta} (y_w | x)}{\pi_{ref} (y_w | x)} - \beta \text{log} \frac{\pi_{\theta} (y_l | x)}{\pi_{ref} (y_l | x)}\Bigr)\Bigr] \end{equation}</p> </li> <li>The authors show that the gradient of the objective w.r.t. the model parameters increases the likelihood of the preferred completion and decreases the likelihood of the not-preferred completion.</li> </ol> <h4 id="yoav-goldbergs-take-on-rl-for-language-modeling"><a href="https://gist.github.com/yoavg/6bff0fecd65950898eba1bb321cfbd81" rel="external nofollow noopener" target="_blank">Yoav Goldbergâ€™s take on RL for language modeling</a></h4> <p>Interesting take suggesting the SFT teaches the model to hallucinate in cases in which the â€œfactâ€ required to answer a question is not contained in the modelâ€™s internal representation. RL on the other hand, allows for â€œnegative reinforcementâ€ by teaching the model when it is wrong. He first introduces the â€œdiversity argumentâ€ which is that SFT is too restrictive in that it prescribes an exact response and penalizes any deviations from it. As humans, we understand that there are likely several â€œequally goodâ€ ways to convey the same information, which is expressible via RL. This is the intuition that I have always had, but Yoav thinks that it is not convincing, because SFT works well in practice and RL is difficult to get right (why do either of these observations make the diversity intuition not convincing?).</p> <h2 id="evaluation">Evaluation</h2> <p>How do we evaluate general-purpose models? Itâ€™s hardâ€¦ especially over multiple turnsâ€¦</p> <h4 id="user-simulation-with-llms-for-evaluating-task-oriented-dialogs"><a href="https://arxiv.org/pdf/2309.13233.pdf" rel="external nofollow noopener" target="_blank">User Simulation with LLMs for Evaluating Task Oriented Dialogs</a></h4> <p>Prompting LLMs &gt; fine-tuning them for user simulation.</p> <h2 id="prompting">Prompting</h2> <p>How to get your model to do watchu want.</p> <h4 id="cot-prompting"><a href="https://arxiv.org/abs/2201.11903" rel="external nofollow noopener" target="_blank">CoT prompting</a></h4> <p>The authors explore â€œchain of thoughtâ€ prompting pretrained LLM models, which simply asks the model to provide step-by-step explanations and provides few-shot examples of such reasoning. A typical use-case in which this is helpful is in grade-school word problems, like â€œQ: Sally has 12 apples. She gave half to John and then ate 3. How many does she have left? A (with CoT): Sally gave half of her apples to John, so she then has 12/2 = 6 apples. She then ate three, so she has 6-3=3 apples.â€ Without CoT, models often hallucinate when asked to provide an answer directly. Their approach remarkably improves performance on many reasoning tasks. They conduct extensive experiments and ablations to investigate why and when CoT is helpful. One interesting finding is that this phenomena is â€œemergentâ€ in that it only improves performance in sufficiently large models.</p> <h4 id="self-consistency-cot-sc"><a href="https://arxiv.org/pdf/2203.11171.pdf" rel="external nofollow noopener" target="_blank">Self Consistency (CoT-SC)</a></h4> <p>Chain-of-thought + self-consitency (CoT-SC): CoT prompting with sampled generation â€“&gt; take the mode response â€“&gt; improved performance</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/sci-figs/cot-sc-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/sci-figs/cot-sc-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/sci-figs/cot-sc-1400.webp"></source> <img src="/assets/img/sci-figs/cot-sc.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Comparison of CoT and CoT-SC from (<a href="https://arxiv.org/pdf/2203.11171.pdf" rel="external nofollow noopener" target="_blank">Wang et al.</a>). </div> <h4 id="tree-of-thought-prompting-tot"><a href="https://arxiv.org/pdf/2305.10601.pdf" rel="external nofollow noopener" target="_blank">Tree of Thought Prompting (ToT)</a></h4> <p>Constructs a â€œtreeâ€ of reasoning paths and allows for search through this space via pruning with an evaluator (an llm with prompts to evaluate states). They apply this to games like 24 improve performance.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/sci-figs/tot-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/sci-figs/tot-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/sci-figs/tot-1400.webp"></source> <img src="/assets/img/sci-figs/tot.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Comparison of ToT and other prompting strategies from (<a href="https://arxiv.org/pdf/2305.10601.pdf" rel="external nofollow noopener" target="_blank">Yao et al.</a>). </div> <h4 id="self-ask"><a href="https://arxiv.org/abs/2210.03350" rel="external nofollow noopener" target="_blank">self-ask</a></h4> <p>Studies the â€œcompositionality gapâ€, a measure of how often models can i) correctly answer all sub-problems required to answer a multi-hop question, but not ii) correctly provide the final solution that requires combining the answers to said subproblems. They propose a prompting strategy to decompose questions to improve multi-hop QA:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/sci-figs/self-ask-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/sci-figs/self-ask-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/sci-figs/self-ask-1400.webp"></source> <img src="/assets/img/sci-figs/self-ask.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Comparison of self-ask and CoT prompting from (<a href="https://arxiv.org/pdf/2210.03350.pdf" rel="external nofollow noopener" target="_blank">Press et al.</a>). </div> <h2 id="time-sensitive-qa">Time-sensitive QA</h2> <p>Answering questions requiring new information!</p> <h4 id="fresh-llm"><a href="https://arxiv.org/abs/2310.03214" rel="external nofollow noopener" target="_blank">fresh-llm</a></h4> <p>Cool tsqa dataset that they claim will update + study of LLMs on these questions + proposed prompting strategy</p> <h2 id="information-retrieval">Information Retrieval</h2> <p>Lots to addâ€¦</p> </div> </article><div id="giscus_thread" style="max-width: 800px; margin: 0 auto;"> <script>let giscusTheme=localStorage.getItem("theme"),giscusAttributes={src:"https://giscus.app/client.js","data-repo":"john-heyer/john-heyer.github.io","data-repo-id":"","data-category":"Comments","data-category-id":"","data-mapping":"title","data-strict":"1","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"bottom","data-theme":giscusTheme,"data-lang":"en",crossorigin:"anonymous",async:""},giscusScript=document.createElement("script");Object.entries(giscusAttributes).forEach(([t,e])=>giscusScript.setAttribute(t,e)),document.getElementById("giscus_thread").appendChild(giscusScript);</script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> </div> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> Â© Copyright 2024 John Heyer. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Last updated: June 28, 2024. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?67f289085adc15e3a89562bf370bd26f"></script> <script defer src="/assets/js/bootstrap-toc.min.js?c82ff4de8b0955d6ff14f5b05eed7eb6"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?07b8786bab9b4abe90d10e61f7d12ff7" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams",inlineMath:[["$","$"],["\\(","\\)"]],processEscapes:!0}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id="></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>