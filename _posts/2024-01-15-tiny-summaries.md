---
layout: post
title: Tiny Paper Summaries
date: 2023-12-15 10:14:00-0400
description: tiny summaries/notes about things I'm reading (to help me remember them)
tags: ml-reading
related_posts: false
toc:
  sidebar: left
---
Forcing myself to write (usually tiny) summaries of things I'm reading to improve retention. Not intended to teach --
will be terse and probably imprecise in places!


## Instruction following
Training models to follow instructions. Sometimes referred to as alignment, but that's an overloaded term imo, and I mean something more specific here. 

#### [LIMA](https://arxiv.org/abs/2305.11206)

The authors fine-tune an LLM (Llama) on a set of just 1K curated instructions from the open assistant dataset. Their model performs comparably to GPT3.5, which was fine-tuned with many more demonstrations + RLHF, and does not trail proprietary models like GPT-4/Claude by much, despite the decrease in size and training data. They coin this phenomenon the “Superficial Alignment Hypothesis”, as their work suggests that the vast majority of LLM capabilities are learned during pretraining rather than during alignment to human preferences via SFT/RLHF/etc. Some interesting findings include: (i) a **negative** correlation between validation perplexity and human-annotated response quality, and (ii) a strong ability to generalize to multi-turn without examples in training or an even stronger performance bump by including as few as 30 multi-turn conversation examples.


#### [Humpback](https://arxiv.org/abs/2308.06259)

The authors present a method for self-aligning language models using entirely open-source data, **without** distilling from data produced by stronger models (i.e., GPT4). They do so with an iterative algorithm that first (i) **augments** the instruction dataset by predicting instructions ($$\hat{x}$$ ) from a assumed fulfillment of said instruction (response $$y$$), which is a span of text from the web. They then (ii) **curate** the set of augmented instruction/response pairs by prompting the model to assign scores to the quality of the instructions, and selecting only those above some score $$k$$. They perform several (2) iterations of augment/curate, and then fine-tune on the augmented, higher quality subset of instruction data, $$A_{k}^{2}$$. The perform exspansive analysis comparing to other instruction-following models, and demonstrate improved performance compared to any model tuned on or distilled from proprietary data.


#### [Unnatural Instructions](https://arxiv.org/abs/2212.09689)

The authors create a set of instruction/input/output data by (i) prompting LLMs to generate instruction/input examples in a few-shot setting with nucleus sampling and (ii) decoding the output greedily with an LLM. Using only LLM generated instruction data, their model performs comparably to an equivalent LLM fine-tuned on “Supernatural Instructions”, which was created using expensive human annotations. Additionally, they provide evidence that suggests that their instruction set is more diverse than those generated by humans (via BERT score), and hypothesize that humans fall back on heuristics or templates learned via annotation guidelines, which results in said lack of instruction diversity.

#### [Vicuna](https://lmsys.org/blog/2023-03-30-vicuna/)

LLaMA fine-tuned on conversations from Share-GPT


#### [Wizard](https://arxiv.org/abs/2304.12244)

LLaMA fine-tuned using instructions of varying complexity generated by an “instruction evolution” (Evol-Instruct) algorithm which prompts a strong LLM (ChatGPT) to make instructions more complex in diverse ways, then filters the pooled instruct/response examples.


#### [Alpaca](https://crfm.stanford.edu/2023/03/13/alpaca.html)

Fine-tuned LLaMA using “[self-instruct](https://arxiv.org/pdf/2212.10560.pdf)” generations produced by ChatGPT given a seed set of instructions.


## RAG & Agents
Getting models to respond using tools + data (potentially) not seen during training (i.e., non-parametrically). 

#### [Toolformer](https://arxiv.org/abs/2302.04761)

The authors detail a fully self-supervised method for teaching an LLM to (selectively) use tools, including calculators, search engines and calendars. They do so by fine-tuning a model on a modified pretraining corpus that includes API input/output results in plain text when relevant. They insert such API calls by (i) few-shot prompting an LLM with examples that demonstrate when an API call is useful(e.g., “One US dollar is approximately equal to 23 Czech crown, or `[API(1/23)→.043]` 4.3%.”), and (ii) filtering to only cases when the API input/output reduces the complexity of the subsequent tokens by at least some fixed amount (hyperparameter). Their model, Toolformer, is better than models which cannot utilize APIs on several tasks including math, question answering, and time-sensitive QA, and even exceeds performance of the much larger GPT3 on several tasks.


#### [self-rag](https://arxiv.org/pdf/2310.11511.pdf)

Learning when to retrieve + critiquing retrieved results generatively


#### [ReAct](https://arxiv.org/abs/2210.03629)

Prior work has explored:
1. using language models to generate actions plans (e.g., [WebGPT](https://arxiv.org/abs/2112.09332))
2. improving "reasoning" capabilities of language models via prompting strategies such as [CoT](https://arxiv.org/abs/2201.11903).
ReAct combines these 2 directions with a prompting "framework" that uses in context learning (ICL) to describe the action space (descriptions of actions/tools/apis) and encourage the model to generate "thoughts" prior to generating action plans. Their hypothesis is that using thoughts will improve reasoning, i.e., action selection in this setting. Their experiments on multi-hop QA, fact-checking, and other interactive decision-making tasks support this.

The authors also explore fine-tuning LLMs on react trajectories (sequences of (thought, action, observation) pairs). They automatically create SFT data via a bootstrapping method in which they use ReAct via few-shot prompting an LLM (not fine-tuned) and using _only_ the trajectories that result in correct final responses (using heuristics like exact match from NQ, for example) for fine-tuning (how biased is this sample?). This also results in improved performance, but at what cost? To what extent are we compromising general language modeling capabilities when we fine-tune on data, most of which essentially only requires solving a classification task (thought/action selection) to model well?

#### [ReWOO](https://arxiv.org/abs/2305.18323)

Removes the sequential (or "reactive" :wink:) planning of ReAct by decoupling the planning from the execution of actions. They do so by generating plans with intermediate results referenced via variables that can be used in subsequent actions (see example in fig below). This is more efficient (time and $), because it requires fewer invocations of the LLM to generate action plans.

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.html path="assets/img/sci-figs/rewoo.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>
<div class="caption">
    Overview of ReWOO from (<a href='https://arxiv.org/pdf/2305.18323.pdf'>Xu et al.</a>).
</div>

#### [LLM Compiler](https://arxiv.org/abs/2312.04511)

Nice work, but glorified ReWOO that fails to give it enough credit. They extend ReWOO by generating a directed acyclic graph (DAG) with their action planner. This allows for parallel execution of independent tasks. This is not really a limitation of ReWOO, just something that the authors did not experiment with. They even mentioned concurrent execution of DAGs in their future work!

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.html path="assets/img/sci-figs/llm-compiler.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>
<div class="caption">
    Overview of LLM Compiler from (<a href='https://arxiv.org/pdf/2312.04511.pdf'>Kim et al.</a>).
</div>


## Parameter-efficient fine-tuning (PEFT)
Fine-tuning models with fewer learned parameters!

#### [LoRA](https://arxiv.org/abs/2106.09685)
The authors fine-tune large models by freezing them and only training small adapters. They do so by decomposing a large matrix $$W'$$ into a sum of the (frozen) pretrained matrix $$W_0$$ and a low-rank perturbation $$\Delta W$$. Thus the forward pass to compute hidden state $$h$$ from input $$x$$, $$h = W_0x$$, is replaced with:
\begin{equation}
h = W_0x + \Delta W_x = W_0x + BAx.
\label{lora-eq1}
\end{equation}
Importantly, $$\Delta W$$ is low rank because it is factorized into matrices $$A$$ and $$B$$ which are dimension $$d_{model} \times r$$, where r is something small like 8. Further, after training, these matrices can be "merged" into a matrix $$W'$$ that's the same size as the original matrix $$W_0$$, so there's no increase in computation/latency at inference time!

During training, they update as few as < 1% of the total parameters, which saves a ton on memory, because optimizers like [Adam](https://arxiv.org/abs/1412.6980) require storing optimizer states proportional in memory to the number of tuned parameters, in order to compute adaptive gradient updates from biased estimates of gradient mean/variance through time.

<div class="row mt-3 justify-content-center">
    <div class="col-sm mt-3 mt-md-0 text-center">
        {% include figure.html path="assets/img/sci-figs/lora.png" class="img-fluid rounded z-depth-1" zoomable=true width="50%" %}
    </div>
</div>
<div class="caption">
    Overview of LoRA from (<a href='https://arxiv.org/pdf/2106.09685.pdf'>Hu et al.</a>). Only A and B matrices are trained.
</div>


#### [QLoRA](https://arxiv.org/pdf/2305.14314.pdf)

The authors fine-tune Llama models using LoRA adapters atop 4 and 8 bit quantized base LLMs without sacrificing performance. Additionally, they fine-tune their model using instruction/chat datasets like open assistant/flanv2 and demonstrate performance comparable to ChatGPT as judged by GPT-4, highlighting the importance of data quality over quantity. Impressively, this can be done using only a single A100 GPU.


## Understanding Transformer Mechanisms
Understanding *how* transformers implement things (mechanistic interpretability) and other transformer-specific stuff.

#### [a mathematical framework for transformers](https://transformer-circuits.pub/2021/framework/index.html)

TODO: I love this work. lot to add on this!

#### [What learning alg is in-context learning?](https://arxiv.org/abs/2211.15661)

The authors demonstrate that via in context learning, transformers are capable of implementing “classic” learning algorithms like using OLS or SGD to solve their prototypical linear regression problems.


## Safety
Understanding + mitigating model biases, frameworks for evaluating models on hard tasks, red-teaming, the works.   

#### [sycophancy](https://arxiv.org/abs/2310.13548)
TODO: briefly describe. 

**Why I like this work**

I think this work is exciting because it provides a way of quantifying _systematic_ failures of LLMs. There are many papers recently that make claims about what LLMs allegedly _can't_ do (e.g., reasoning), but too often the goal often seems to be to criticize existing methods or claims without offering solutions. This work instead seeks to identify situations in which LLMs stray from the truth, but suggests that there are ways to mitigate this issue! Further, sycophancy in models is intuitive and perhaps not surprising, but measuring it is quite complicated; this paper proposes creative ways of designing experiments to analyze a specific behavior. I am hopeful that additional work in identifying, and root causing, other systematic failures might lead to measurable improvements toward reliable and truthful assistants. My hunch is that preference models optimize for "style" which is often at odds with truth, suggesting that we need to find ways to model for this outside of simply scaling data and compute.

#### [scalable oversight](https://arxiv.org/abs/2211.03540)

:sandwich:

## RL + Human Preferences
Learning from human preference data?!

#### [RLHF (Instruct GPT)](https://arxiv.org/pdf/2203.02155.pdf)

Collect preference data by asking humans which completions they prefer — can be pairwise, or a full ranking over k completions. Using this preference data, train a reward model to compute scores for each completion independently. This score is used in the loss function by taking k choose 2 pairs and ensuring the preferred generation A is given a higher score than less-preferred generation B as follows:

  $$

  \text{Loss}(\theta) = -1/(k  \text{c} 2)\mathbb{E}[\text{log}(\sigma(r_{\theta}(x, y_{win}) - r_{\theta}(x, y_{loss})))]

  $$

Then the model is trained using RL, PPO specifically, to maximize the reward for *new* completions to inputs by sampling completions and updating the policy to maximize expected reward returned by the reward model. Additionally, there is a KL-divergence component of the loss wrt the original model (”policy”), to regularize the model to ensure it retains most of the pre-trained knowledge.


#### [PPO](https://openai.com/research/openai-baselines-ppo)

Policy-gradient RL learning alg (sample trajectories, then compute the gradient wrt the expected reward). Has some additional features to reduce the variance of the gradient vector (variance in gradient update is measured across minibatches? increasing batch-size reduces variance between mini-batches — rl suffers more from this problem because there isn’t direct supervision, but rather random trajectories through the environment).


#### [DPO](https://arxiv.org/abs/2305.18290)
**tl;dr**: Shows that you can reformulate the reward maximizing objective used in RLHF as a function of your current parameterized policy (i.e., your language model).  In essence, learn to assign higher log likelihood to the preferred completion than the non-preferred one, as opposed to training a separate reward model.

**From RLHF to the DPO Objective**

1. Assume human preferences are generated by some latent reward model, $$ r^*(x, y) $$
2. The Bradley-Terry preference model defines the preference distribution as a function of these rewards:
\begin{equation}
\label{dpo-eq1}
p(y_1 \succ y_2 | x) = \frac{\text{exp}(r^{\*}(x, y_1))}{\text{exp}(r^{*}(x, y_1)) + \text{exp}(r^{\*}(x, y_2))}
\end{equation}

3. RLHF first trains a reward model to maximize the difference between the rewards assigned to the winning and losing completions:
\begin{equation}
\label{dpo-eq2}
\text{Loss}(\phi) = -\mathbb{E}[\text{log}\sigma(r_{\phi}(x, y_{w}) - r_{\phi}(x, y_{l}))]
\end{equation}

4. Then trains a language model to maximize the rewards from this model:
\begin{equation}
\label{dpo-eq3}
\text{max}\_{\pi_{theta}} \mathbb{E}[r_{\phi}(x, y)]  -\beta \text{D}\_{\text{KL}} [\pi_{theta}(y | x)  || \pi_{\text{ref}}(y | x)],
\end{equation}
where $$ \text{D}_{\text{KL}} $$ is the KL-divergence between 2 distributions, defined by the current policy and the "reference" policy, typically defined by the model prior to RLHF training (i.e., the initialized model). This is a form of regularization that also prevents mode collapse.

5. DPO first shows that the optimal solution to the RLHF objective has the form:
\begin{equation}
\label{dpo-eq4}
\pi_{r}(y | x) = \frac{1}{Z(x)} \pi_{\text{ref}}(y | x) \text{exp}(\frac{1}{\beta}r(x, y),
\end{equation}
where $$ Z(x) $$ is the partition function, or normalizing constant required to convert the above into a valid probability distribution. This is intractable to compute because it'd require summing over all generations $$ y $$.

6. We can now re-write the reward as a function of its optimal policy:
\begin{equation}
\label{dpo-eq5}
r(x, y) = \beta \text{log} \frac{\pi_r (y | x)}{\pi_{ref} (y | x)} + \beta \text{log} Z(x)
\end{equation}

7. There are 2 important things here: i) the above is a function of the optimal _policy_, which we parameterize directly (the LLM), rather than a separate reward model, and ii) the partition term cancels out when we take the difference of two rewards, which is all that matters for learning a reward model from preference data. Now, we can plug this reward difference into the reward modeling objective equation \eqref{dpo-eq2}, and optimize this by tuning our generative model's weights directly:
\begin{equation}
\label{dpo-eq6}
\mathcal{L}\_{\text{DPO}} = -\mathbb{E}\Bigl[\text{log}\sigma\Bigl(\beta \text{log} \frac{\pi_{\theta} (y_w | x)}{\pi_{ref} (y_w | x)} - \beta \text{log} \frac{\pi_{\theta} (y_l | x)}{\pi_{ref} (y_l | x)}\Bigr)\Bigr]
\end{equation}

8. The authors show that the gradient of the objective w.r.t. the model parameters increases the likelihood of the preferred completion and decreases the likelihood of the not-preferred completion.

#### [Yoav Goldberg's take on RL for language modeling](https://gist.github.com/yoavg/6bff0fecd65950898eba1bb321cfbd81)

Interesting take suggesting the SFT teaches the model to hallucinate in cases in which the “fact” required to answer a question is not contained in the model’s internal representation. RL on the other hand, allows for “negative reinforcement” by teaching the model when it is wrong. He first introduces the “diversity argument” which is that SFT is too restrictive in that it prescribes an exact response and penalizes any deviations from it. As humans, we understand that there are likely several “equally good” ways to convey the same information, which is expressible via RL. This is the intuition that I have always had, but Yoav thinks that it is not convincing, because SFT works well in practice and RL is difficult to get right (why do either of these observations make the diversity intuition not convincing?).


## Evaluation
How do we evaluate general-purpose models? It's hard... especially over multiple turns...

#### [User Simulation with LLMs for Evaluating Task Oriented Dialogs](https://arxiv.org/pdf/2309.13233.pdf)

Prompting LLMs > fine-tuning them for user simulation.

## Prompting
How to get your model to do watchu want.

#### [CoT prompting](https://arxiv.org/abs/2201.11903)

The authors explore “chain of thought” prompting pretrained LLM models, which simply asks the model to provide step-by-step explanations and provides few-shot examples of such reasoning. A typical use-case in which this is helpful is in grade-school word problems, like “Q: Sally has 12 apples. She gave half to John and then ate 3. How many does she have left? A (with CoT): Sally gave half of her apples to John, so she then has 12/2 = 6 apples. She then ate three, so she has 6-3=3 apples.” Without CoT, models often hallucinate when asked to provide an answer directly. Their approach remarkably improves performance on many reasoning tasks. They conduct extensive experiments and ablations to investigate why and when CoT is helpful. One interesting finding is that this phenomena is “emergent” in that it only improves performance in sufficiently large models.

#### [Self Consistency (CoT-SC)](https://arxiv.org/pdf/2203.11171.pdf)
Chain-of-thought + self-consitency (CoT-SC): CoT prompting with sampled generation --> take the mode response --> improved performance


<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.html path="assets/img/sci-figs/cot-sc.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>
<div class="caption">
    Comparison of CoT and CoT-SC from (<a href='https://arxiv.org/pdf/2203.11171.pdf'>Wang et al.</a>).
</div>

#### [Tree of Thought Prompting (ToT)](https://arxiv.org/pdf/2305.10601.pdf)
Constructs a "tree" of reasoning paths and allows for search through this space via pruning with an evaluator (an llm with prompts to evaluate states). They apply this to games like 24 improve performance.

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.html path="assets/img/sci-figs/tot.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>
<div class="caption">
    Comparison of ToT and other prompting strategies from (<a href='https://arxiv.org/pdf/2305.10601.pdf'>Yao et al.</a>).
</div>

#### [self-ask](https://arxiv.org/abs/2210.03350)
Studies the "compositionality gap", a measure of how often models can i) correctly answer all sub-problems required to answer a multi-hop question, but not ii) correctly provide the final solution that requires combining the answers to said subproblems.
They propose a prompting strategy to decompose questions to improve multi-hop QA:


<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.html path="assets/img/sci-figs/self-ask.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>
<div class="caption">
    Comparison of self-ask and CoT prompting from (<a href='https://arxiv.org/pdf/2210.03350.pdf'>Press et al.</a>).
</div>

## Time-sensitive QA
Answering questions requiring new information!

#### [fresh-llm](https://arxiv.org/abs/2310.03214)

Cool tsqa dataset that they claim will update + study of LLMs on these questions + proposed prompting strategy

## Information Retrieval

Lots to add...

